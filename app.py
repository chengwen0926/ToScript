import os
import sys
import argparse
import gradio as gr
import numpy as np
import random
# import torch
# import torchaudio
from pathlib import Path
ROOT_DIR = os.path.dirname(os.path.abspath(__file__))


inference_mode_list = ['éŸ³é¢‘æ¨ç†', 'å›¾åƒæ¨ç†', 'è§†é¢‘æ¨ç†']
instruct_dict = {
    inference_mode_list[0]: '1. é€‰æ‹©\'{}\'å¹¶ä¸Šä¼ æºéŸ³é¢‘\n2. ç‚¹å‡»ç”Ÿæˆè„šæœ¬æŒ‰é’®'.format(inference_mode_list[0]),
    inference_mode_list[1]: '1. é€‰æ‹©\'{}\'å¹¶ä¸Šä¼ æºå›¾åƒ\n2. ç‚¹å‡»ç”Ÿæˆè„šæœ¬æŒ‰é’®'.format(inference_mode_list[1]),
    inference_mode_list[2]: '1. é€‰æ‹©\'{}\'å¹¶ä¸Šä¼ æºè§†é¢‘\n2. ç‚¹å‡»ç”Ÿæˆè„šæœ¬æŒ‰é’®'.format(inference_mode_list[2])
}
stream_mode_list = [('æ˜¯', True), ('å¦', False)]

def change_mode(mode_checkbox_group):
    aud_visible = {
        "__type__": "update",
        "visible": mode_checkbox_group==inference_mode_list[0]
    }
    img_visible = {
        "__type__": "update",
        "visible": mode_checkbox_group==inference_mode_list[1]
    }
    vid_visible = {
        "__type__": "update",
        "visible": mode_checkbox_group==inference_mode_list[2]
    }

    return instruct_dict[mode_checkbox_group] , aud_visible, img_visible, vid_visible

def generate_script():
    import time
    content = 'ã€Šå“ªå’ï¼šé­”ç«¥é™ä¸–ã€‹è®²è¿°äº†ä¸€ä¸ªé¢ è¦†ä¼ ç»Ÿçš„ç¥è¯æ•…äº‹ã€‚å¤©åœ°çµæ°”å­•è‚²å‡ºæ··å…ƒç ï¼Œå…ƒå§‹å¤©å°Šå°†å…¶åˆ†ä¸ºçµç å’Œé­”ä¸¸ï¼Œçµç æŠ•èƒä¸ºå“ªå’ï¼Œé­”ä¸¸åˆ™è¢«æ–½ä»¥å¤©åŠ«å’’ï¼Œä¸‰å¹´åå°†è¢«å¤©é›·æ‘§æ¯ã€‚ç„¶è€Œï¼Œçµç è¢«ç”³å…¬è±¹ç›—èµ°ï¼ŒæŠ•èƒä¸ºé¾™ç‹ä¹‹å­æ•–ä¸™ï¼Œè€Œå“ªå’åˆ™è¢«è¯¯è®¤ä¸ºæ˜¯é­”ä¸¸è½¬ä¸–ã€‚å“ªå’ä»å°è¢«è§†ä¸ºå¦–æ€ªï¼Œå¤‡å—æ­§è§†ï¼Œä½†ä»–å†…å¿ƒæ¸´æœ›è¢«è®¤å¯â€¦'
    response = ''
    for char in content:
        response += char
        yield response  # æ¯æ¬¡ yield ä¸€ä¸ªå­—ç¬¦
        time.sleep(0.1)

def output_directly(para):
    return para

def main():
    theme = gr.themes.Soft()
    block = gr.Blocks(theme=theme).queue()
    with block as demo:
        gr.Markdown("### ä»£ç åº“ [ToScript](https://github.com/chengwen0926/ToScript) \
                    é¢„è®­ç»ƒæ¨¡å‹ [Whisper-Large-v3-Turbo](https://huggingface.co/openai/whisper-large-v3-turbo)")
        gr.Markdown("#### è¯·é€‰æ‹©æ¨ç†æ¨¡å¼ï¼Œå¹¶æŒ‰ç…§æç¤ºæ­¥éª¤è¿›è¡Œæ“ä½œ")

        with gr.Row():
            mode_checkbox_group = gr.Radio(choices=inference_mode_list, label='é€‰æ‹©æ¨ç†æ¨¡å¼', value=inference_mode_list[0])
            stream = gr.Radio(choices=stream_mode_list, label='æ˜¯å¦æµå¼æ¨ç†', value=stream_mode_list[0][0])
            instruction_text = gr.Text(label="æ“ä½œæ­¥éª¤", value=instruct_dict[inference_mode_list[0]])
        
        with gr.Accordion(label='æ¨¡å‹åŠå…¶ä»–é…ç½®ä¿¡æ¯', open=False) as configuration:
            with gr.Row():
                with gr.Column():
                    gr.Radio(["Audio2Script", "Image2Script", "Video2Script"], value="Audio2Script", label='æ¨¡å‹ç±»å‹')
                with gr.Column():
                    gr.Textbox(placeholder='Input HuggingFace Repository ID to Download', label='ğŸ¤— Repo ID', interactive=True)
                with gr.Column():
                    gr.Button(value="æ¨¡å‹ä¸‹è½½&åŠ è½½éƒ¨ç½²")

           

        with gr.Row() as aud_upload_row:
            aud_upload = gr.Audio(sources="upload", type='filepath', label='é€‰æ‹©éŸ³é¢‘æ–‡ä»¶ï¼Œæ³¨æ„é‡‡æ ·ç‡ä¸ä½äº16khz')
        with gr.Row(visible=False) as img_upload_row:
            img_upload = gr.Image(sources="upload", type='filepath', label='é€‰æ‹©å›¾åƒæ–‡ä»¶')
        with gr.Row(visible=False) as vid_upload_row:
            vid_upload = gr.Video(sources="upload", label='é€‰æ‹©è§†é¢‘æ–‡ä»¶')
        

        generate_button = gr.Button("ç”Ÿæˆæ–‡æœ¬")
        script_output = gr.Textbox(label='æ£€æµ‹åˆ°çš„è„šæœ¬å†…å®¹', lines=10, autoscroll=True, interactive=False)        
        mode_checkbox_group.change(fn=change_mode, inputs=[mode_checkbox_group], outputs=[instruction_text, aud_upload_row, img_upload_row, vid_upload_row])
        generate_button.click(fn=generate_script,inputs=[],outputs=[script_output])

        # Gradioä¸­çš„Videoå¯¹è±¡ä½œä¸ºè¾“å…¥æ—¶æŠ¥é”™TypeError: argument of type 'bool' is not iterable
        # æ›´æ–° pip install pydantic==2.10.6
        vid_upload.upload(fn=output_directly,inputs=[vid_upload],outputs=[script_output])

    demo.queue(max_size=4, default_concurrency_limit=2)
    demo.launch(server_name='127.0.0.1', share=True, server_port=8090)

if __name__ == '__main__':
    # parser = argparse.ArgumentParser()
    # parser.add_argument('--port',
    #                     type=int,
    #                     default=7890)
    # parser.add_argument('--model_dir',
    #                     type=str,
    #                     default='pretrained_models/whisper-large-v3-turbo',
    #                     help='local path or modelscope repo id')
    # args = parser.parse_args()
    # try:
    #     cosyvoice = CosyVoice(args.model_dir)
    # except Exception:
    #     try:
    #         cosyvoice = CosyVoice2(args.model_dir)
    #     except Exception:
    #         raise TypeError('no valid model_type!')

    # sft_spk = cosyvoice.list_available_spks()
    # if len(sft_spk) == 0:
    #     sft_spk = ['']
    # prompt_sr = 16000
    # default_data = np.zeros(cosyvoice.sample_rate)
    main()